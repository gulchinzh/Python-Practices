{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.select_dtypes('object').columns:\n",
    "    #if len(df[i].unique())<100:\n",
    "        print(i , \"\\n\"\"\\n\", df[i].value_counts(),\"\\n\",\"\\n\",\"**********************************\"\"\\n\\n\")\n",
    "    #else :\n",
    "        #print(\"\\n\",\"***\",i,\"***\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbce7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.select_dtypes(['int64','float64']).columns:\n",
    "    if len(df[i].unique())<100:\n",
    "        print(i , \"\\n\"\"\\n\", df[i].value_counts(dropna = False),\"\\n\",\"\\n\",\"**********************************\"\"\\n\\n\")\n",
    "    else :\n",
    "        print(\"\\n\",\"***\",i,\"***\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df[\"gender\"] == \"Other\")].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from yellowbrick.features import RadViz\n",
    "plt.rcParams[\"figure.figsize\"] = (9,5)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45764135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot = True)\n",
    "#çok yüksek correlasyon var demek o feature ın önemli olduğu anlamına gelmiyor. feature'ları le corr.lar ile çarparak (ve intercept ekleyerek) karar verebiliriz.\n",
    "\n",
    "df_numeric.corr()[(df_numeric.corr()>= 0.9) & (df_numeric.corr() < 1)].any().any() \n",
    "\n",
    "# ilk any nümerik columların corr larında 0.9 - 1 arasında olanlarını True döndürür. diğerlerini False döndürür\n",
    "# ikinci any ile ilk any'nin result'ı üzerinde True kontrolü yapmış oluyoruz. yani ikinci any ile kullandığımızda True varsa\n",
    "# tek any'li haline bakıp hangileri True ona bakabiliriz. iki any'li halinde True yoksa demek ki hiç true yoktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='bmi', y='age', data=df)\n",
    "sns.histplot(df['price'], bins = 50, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a33650",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df.price)\n",
    "---------------------------------------------------------\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(x=\"make_model\", y=\"price\", data=df, whis=3)\n",
    "plt.show()\n",
    "#outlierlara baxmaq ucun kodlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f560a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bmi.fillna(df['bmi'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('stroke', axis =1) # datasetimizden 'stroke' target imizi atiriq, featurlerimiz qalir onu da X a beraber edirik\n",
    "y = df['stroke'] # 'stroke' targetimizi y beraber edirik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select_dtypes(include =['object'])  # yada (include =['int64'])\n",
    "# data type lari object -- categorical olan feauterleri getir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.isna().any()].tolist() # icinde NaN value lari olan featurelerin adlarini getirir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select_dtypes(include =\"object\").head()\n",
    "\n",
    "# datasetimdeki object feature ları getirmesi için ,dataframe formasinda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('object').columns #sadece featurelarin adlarini verir\n",
    "df_numeric = df.select_dtypes(include =\"number\") # yalniz numeric featurelari verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('object').columns:\n",
    "    print(f\"{col:<20}:\", df[col].nunique())  # col:<20 aşağıda print result'ı olarak sütun gernişliğini ayarlıyor\n",
    "    \n",
    "# aşağıdaki unique değer sayısı yüksek olan 4 column ile missing value lar ile mücadele kapsamında ciddi anlamda mücadele etmek gerekiyor ama bu dersin konusu bu değil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.make_model==\"Audi A2\"]\n",
    "\n",
    "# drop edeceğim Audi A2'nin satır numarasını öğrendim\n",
    "df.drop(index = [2614], inplace = True)\n",
    "\n",
    "# Audi A2'yi drop ettim\n",
    "\n",
    "# ya da\n",
    "df = df[df.gender != 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19605501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.make_model.unique():\n",
    "    print(i, \"\\t:\", skew(df[df[\"make_model\"] ==i][\"price\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.dropna(subset = [\"driver_gender\"], inplace = True)\n",
    "# subset kullanmadan dropna yaptığımda NaN olan değerleri düşürünce içinde NaN değer olan tüm sütunların satırları \n",
    "#gitmiş olacak. ama o satırda içinde değerli veri olan sütunlar da olabilir. Bunu subset ile yapacağız ve sadece\n",
    "#belirttiğimiz sütundaki NaN olan değerlerin satırlarını silecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d142a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri[\"is_arrested\"] = ri.is_arrested.astype(\"bool\")\n",
    "\n",
    "# ri[\"is_arrested\"] serie sini içeriğini bool yapıp kendisine eşitliyorum. yani bool'a değiştirmiş oluyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b747b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#İki pandas serisini str.cat() fonksiyonunu kullanarak birleştireceğim.\n",
    "#stop_date'i .str.cat()' in içine pas edeceğim ri.stop_time ile birleştireceğim.\n",
    "#ve bunu combined adlı değişkene atayacağım.\n",
    "\n",
    "combined = ri.stop_date.str.cat(ri.stop_time, sep = \" \")\n",
    "ri[\"stop_datetime\"] = pd.to_datetime(combined)\n",
    "\n",
    "# şimdi eski iki sütunu drop edelim.\n",
    "\n",
    "ri.drop([\"stop_date\", \"stop_time\"], axis = \"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.set_index(\"stop_datetime\", inplace = True)\n",
    "\n",
    "# stop_datetime sütunumu index yaptım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_and_speeding = ri[(ri.driver_gender == \"F\") & (ri.violation == \"Speeding\")]\n",
    "\n",
    "# hem female olanları hem de violationı sadece speeding olanları getir.\n",
    "# artık female_and_speeding bu bilgilerden oluşan satırları getirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! normalize mantığı şunla aynıdır:\n",
    "\n",
    "# male.violation.value_counts(normalize = True)\n",
    "# male.violation.value_counts() / male.violation.value_counts().sum()\n",
    "\n",
    "#ya da :\n",
    "\n",
    "#for i in unique_vol:\n",
    "    print(i / unique_amount * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)  \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    R2_score = r2_score(actual, pred)\n",
    "    print(\"Model testing performance:\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"R2_score \\t: {R2_score}\")\n",
    "    print(f\"MAE \\t\\t: {mae}\")\n",
    "    print(f\"MSE \\t\\t: {mse}\")\n",
    "    print(f\"RMSE \\t\\t: {rmse}\")\n",
    "y_train_pred = lm.predict(X_train)\n",
    "eval_metrics(y_train, y_train_pred)\n",
    "y_pred = lm.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alpha_space = np.linspace(0.01, 1, 100)\n",
    "ridge_cv_model = RidgeCV(alphas = alpha_space, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "ridge_cv_model.fit(X_train, y_train)\n",
    "y_pred = ridge_cv_model.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)\n",
    "y_train_pred = ridge_cv_model.predict(X_train)\n",
    "eval_metrics(y_train, y_train_pred) \n",
    "\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "lasso_model = Lasso(alpha = 1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_train_pred = lasso_model.predict(X_train)\n",
    "eval_metrics(y_train, y_train_pred)\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)\n",
    "\n",
    "lasso_cv_model = LassoCV(alphas = alpha_space, cv = 10, max_iter = 100000) \n",
    "lasso_cv_model.fit(X_train, y_train)\n",
    "y_train_pred = lasso_cv_model.predict(X_train)\n",
    "eval_metrics(y_train, y_train_pred)\n",
    "y_pred = lasso_cv_model.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)\n",
    "lasso_cv_model.coef_\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "elastic_cv_model = ElasticNetCV(alphas = alpha_space, cv = 10, max_iter = 100000)\n",
    "elastic_cv_model.fit(X_train, y_train)\n",
    "y_train_pred = elastic_cv_model.predict(X_train)\n",
    "eval_metrics(y_train, y_train_pred)\n",
    "y_pred = elastic_cv_model.predict(X_test)\n",
    "eval_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train) \n",
    "x_test = scaler.transform(x_test)\n",
    "# burada x_traine fit_transform, x_teste transform edirik ki data leak ici engellemek ucun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean().iplot(kinde ='bar', subplots =True) # Null value larin mean ini bar plot seklinde visual gosterir, \n",
    "                                                       # cok ise yaraya bilir\n",
    "msno.bar(df); # bu kodlari yuxaridakinin altindan da yaza bilerik istersek, nerelerde variable llarin kayip oldugunu ya da olmadigini gormus oluruq\n",
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean()) # nall valuelari mean le doldurmaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eger data mizda skew ness varsa her zaman safe side da qalamq icin <median> ni kullanmak lazim, null velu lari doldurmaq ucun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'].value_counts(normalize = True) # normalize true yuzdesine baxir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ama evvel iplot un librarisini import etmek lazim\n",
    "\n",
    "1-- df['col'].iplot(kind = 'hist') # rahat sunum yapa bilmemiz icin gorsel\n",
    "\n",
    "\n",
    "2-- df['col'].iplot(kind = 'histogram', subplots = True, bins =50) # rahat sunum yapa bilmemiz icin gorsel\n",
    "\n",
    "3-- df.iplot(subplots =True, subplots_titles =True,legend =False, shape(2,5) ,kind ='histogram')  # rahat sunum yapa bilmemiz icin gorsel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missin value larla ilgili kode🎈 cok ise yararli, cedvel seklinde hem yuzdesini, hem de numberini gosterir\n",
    "def missing (df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending =False)\n",
    "    missing_percent = df.isnull().sum() / df.isnull().count().sort_values(ascending = False)\n",
    "    missing_values = pd.concat([missing_number,missing_percent], axis =1 , keys = ['Missing_Number' , 'Missing_Percent'])\n",
    "    return missing_values\n",
    "missing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topla_10 (x):\n",
    "    return x + 10\n",
    "\n",
    "df.apply(topla_10) # yazdiqda her columndaki deyerlerin uzerine 10 elave edir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a85789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cixma (x):\n",
    "    return x['A_col'] - x['B_col']\n",
    "\n",
    "df.apply(cixma, axis =1) # A_col dan B_col u cixir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df.loc[:, 'Street Number Suffix':'Zipcode'].head() # icinde missing valuelari olan columnlara baxmaq ucun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410febc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_imputed = df.fillna(method='bfill', axis=0).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['date'].head()) #column un headine de baxa bilirik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but the most common are %d for day, %m for month, %y for a two-digit year and %Y for a four digit year.\n",
    "\n",
    "# Some examples:\n",
    "\n",
    "# 1/17/07 has the format \"%m/%d/%y\"\n",
    "# 17-1-2007 has the format \"%d-%m-%Y\"\n",
    "\n",
    "df['date_parsed'] = pd.to_datetime(df['date'], format=\"%m/%d/%y\") \n",
    "# date column unun dtype i 'Object' idi, ama icindeki deyerler tarix formasinda yazilmisdi,\n",
    "# 'Object' dtype i datetime dtype a convert etdik, column un adini date_parsed yazdiq\n",
    " \n",
    "# sometimes you'll run into an error when there are multiple date formats in a single column.\n",
    "df['date_parsed'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
    "# infer_datetime_format = True ---> bir column da multiple date formatlar var ise error olmasinin qarsisini alir\n",
    "\n",
    "df = df['date_parsed'].dt.day\n",
    "# kalendardan yalniz aylari getirir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # ne kadar duplicat deyer var\n",
    "df[df.duplicated()] # duplicat deyerleri dataframe kimi gosterir\n",
    "df.drop_duplicates(inplace=True) # duplicat leri silmek\n",
    "\n",
    "# column lari drop etmezden once bir veriable in icinde list yaradib adlarini ora yaziriq\n",
    "drop_columns = ['col1','col2','col3','col4']\n",
    "df.drop(drop_columns, axis = 1, inplace =True) # axis = 'columns' axis = 1 ile eyni seyi verir\n",
    "# tek bir sutun dusureceksek ayrica list yaratmaga gerek yok\n",
    "df.drop(['col'], axis =1, inplace =True) \n",
    "\n",
    "# her hansi bir column u DataFramemize index ede bilerik...\n",
    "df.set_index('col_name', inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame i biz list , dictionary , nympy arrayle olustura bilerik\n",
    "df.head() # ilk 5 row u getirir default u 5 dir\n",
    "df.tail()# son 5 row u getiri\n",
    "df.sample() # datamizin icinden herhansi bir row u getirir, icine ne reqemi yazarsaq o qeder row getirir\n",
    "df.columns # datamizin icindeki column adlarini getirir\n",
    "df.shape() # datamizin nece row ve columndan olustugunu getirir\n",
    "df['col_name'].values() # columnun icindeki deyerleri getirir\n",
    "df['new_col'] = df['a_col'] + df['b_col'] # yeni column duzeltdikde iki col u toplaya cixa vura bole bilerik\n",
    "# fazlalik yapmamasi icin df['a'] ve df['b']  df.drop('col_name', axis =1)\n",
    "df.drop('col_name', axis =1) # col u silmek ucun\n",
    "df.drop(sira_nomresi , axis =0) # kalici olmasini isteyirikse inplace = True\n",
    "# indexle bagli drop etdikden sonra reset_index etmek lazimdir\n",
    "df.reset_index(drop = True) # qalici olmasi ucun de drop = True yaziriq\n",
    "df.loc[2] # sira nomresi 2 olan row u getir\n",
    "df.loc[2:5] # sira nomresi 2 ve 5 olan(5 daxildir) rowlari getir\n",
    "df.iloc[2:5] # sira nomresi 2 ve 5 olan(5 deyil) rowlari getir\n",
    "df.iloc[2:5, 3:5] # 2:5 1ci rowlari secirik, 2ci 3:5 columnlari\n",
    "df.loc[2:5]['a_col'] # ['a_col'] columundaki sira nomresi 2 ve 5 (daxildir) arasinda olan deyerleri getir . Serie formasinda\n",
    "df.loc[2:5][['a_col']] # kvadrat motereze iki qat olanda DFrame olur\n",
    "df.loc[index num][['col_name']] 😉\n",
    "df.loc[: ,[['a_col']]] # a_col un row larinin hamisini al\n",
    "df.['a_col'] # buda columun butun row larini getirir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab946978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteringi & | !=  == > < edirik\n",
    "\n",
    "df[(df['a_col'] > 0) & (df['b_col'] > 2)] # a_col da 0 dan boyuk, b_col da 2 den boyuk olan deyerleri getirir\n",
    "df[(df['a_col'] > 0) & (df['b_col'] > 2)].shape\n",
    "df[(df[col1]>2) & (df[col2]==222)]\n",
    "df[df['price'] > 300 ] # 'price' i 300 den yuxari olanlari getir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cffa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'].min() # col un icindeki min deyeri verir (max,std,mean,max,mode,median da qoya bilerik)\n",
    "df['col'].idxmin() # col un minimum deyerinin index numberini verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('col_1')['col2'].mean() # col1 i col2 ye gore groupla ve onlarin mean(max,std,mean,max,mode,median da qoya biler)\n",
    "df.groupby('col_1')['col2'].describe().T['Value'] # 'col_1 in icindeki her hansi bir Value ya baxa bilerik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c571eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col.unique() # sutunun icinde nece eded benzersiz(unikal) deyer\n",
    "df.col.nunique() # unikal deyerlerin sayini verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'col_1', asending = False) # col_1 in value larini siralayir , False oldugu ucun coxdan aza, True olsaydi azdan coxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e563b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eger dataset de NaN value lar varsa butun NaN lari hello ile doldurur\n",
    "df.fillna('hello')\n",
    "\n",
    "# column un NaN valuelarini mean, median, mode la doldura bilirik\n",
    "df['col2'].fillna(df.col2.mean())\n",
    "df['col'].fillna('bfill') # backfill ozunden sonrakiyla doldurur\n",
    "df['col'].fillna('ffill') # frontfill ozunden sonrasiyla doldurur\n",
    "df['colf'].fillna(method='ffill', limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lamda x: x.fillna(x.mean()), axis =0) # setir boyunca missing valu lari mean le doldurur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = df['col'].map(lambda x: x*10) # 'col' un icindeki deyerleri lambda ile 10 a vurub, map ile 'col'un icine qoyuruq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c145590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'] = df['company'].replace('Goo','Google') # 'company' columnun icinde harada Goo gorse Google la evezle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca653b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'] = df['company'].astype(str) # column un typeni str ya deyisdik\n",
    "\n",
    "# columnlari birlesdirmek...\n",
    "combined = df.col1.str.cat(df.col2, sep = '') # str.cat() function la 2 tane serie imi (df.col1 ve df.col2) birlesdiririk\n",
    "                                            # bir-birine yapismasin deye sep ='' arasina bosluq qoyuruq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc74ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function\n",
    "\n",
    "def multiply (x):\n",
    "    return x * 10\n",
    "\n",
    "df['col'] = df['col'].apply(multiply)\n",
    "\n",
    "# yuxarida yaratdigimiz multiply functionunu asagida apply la 'col' un value larina tetbiq etdik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('col1')['col2'].mean()  # 'col1' i 'col2' nin ortalamasina gore qrupla\n",
    "\n",
    "df.groupby(['col1','col2'])['col3'].mean() # 'col1' ve 'col2' ni 'col3' un ortalamsina gore qruplandir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63034f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name'].value_counts() # icinde hansi deyerden nece eded oldugunu gosterir\n",
    "sns.countplot(df['col_name']) # value_counts() kimidir visual olaraq gosterir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007be682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull.sum()  # datamizdaki her columnun ayri-ayri resultini verir\n",
    "df.isnull().any()  # datamizdaki deyerlerin NaN oldugunu boolin olaraq True yada False getirir\n",
    "df.isnull().sum().sum()  # datamizdaki NaN value lar hansi columnlardadirsa onlarin umumi cemini getirir\n",
    "\n",
    "df.col_A.value_counts(dropna = False) # col_A sutununun icindeki butun deyerleri ve her deyerden nece eded oldugunu getirir\n",
    "                                        # dropna = False olanda onun icindeki NaN larin sayini da getirir\n",
    "    \n",
    "df.col_A.value_counts(normalize = True) # cikan deyerleri 0 ve 1 arasinda normalize edir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fe055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0utlier\n",
    "sns.boxplot(x = np.log(df['col_name'])) # bu kod saga skew olan datalarcun istifade olunur,\n",
    "                                        # sagdaki outlayerleri temizleyende log funtion dan ist olun\n",
    "\n",
    "# outlier lerden temizlemek ucun basqa yontem TuKey methodu\n",
    "sns.boxplot(x = df['col_name']) # hansi columdan subhelenmisikse boxplotla onu sekillendirib, varsa eger temizleyirik\n",
    "\n",
    "Q1 = df['col_name'].quantile(0.25)\n",
    "Q3 = df['col_name'].quantile(0.75)\n",
    "IQR = Q3 -Q1\n",
    "\n",
    "lover_lim = Q1 - 1.5 * IQR\n",
    "upper_lim = Q2 - 1.5 * IQR\n",
    "\n",
    "df[(df['col_name'] < lover_lim) | (df['col_name'] > upper_lim)] # outlier lari olan row lari getirir\n",
    "\n",
    "df[~((df['col_name'] < lover_lim) | (df['col_name'] > upper_lim))] # eyni kodun eveline tilda~ yazarsaq ve elave moterezeye \n",
    "                                                                # alarsaq, outlier leri olmayan rowlari getirir\n",
    "\n",
    "# basqaq bir yontem de Winsorize methodu dur, evvel import etmemiz lazim\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "df_winsorize = winsorize(df['col_name'],(0.01 , 0.02)) # bu reqemlerin arasinda outliyer olmayan deyerleri al misal ucun\n",
    "# yeniden boxplotla baxiriq temizlenib ya yox\n",
    "sns.boxplot(x = df_winsorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat   # mergle concat pandasin methodlaridir\n",
    "\n",
    "pd.concat([df1,df2,df3] , ignore_index =True) # concatdan sonra shit tab yapsak iterable deyer istiyor, \n",
    "                                                #o yuzden df leri list icinde yazdiq\n",
    "                                            # concatin defaultu axis = 0 , row lar uzerinden birlesdirir\n",
    "                                            # ignore_index =True indexleri 0 dan basla sirala\n",
    "\n",
    "pd.concat([df1,df2,df3], axis =1) # columnlari yan yana qoyub yapisdirsaq index df1,df2,df3 un index numberler eynidirse\n",
    "                            # problem olmaz yox eger ferqlidirse, ust uste dusmeyen yerler bos qalacaq onlar da NaN olacaq\n",
    "    \n",
    "# merge \n",
    "\n",
    "pd.merge(df1_left,df2_right, how = 'inner', on = 'ortaq_1_col') # df1_left ve df2_right datasetlerinin ortaq 1 column \n",
    "                                                              # uzerinden merge edir\n",
    "# how = 'inner'in yerine 'outer' ~ 'right' ~ 'left'da yazabilerik ,buneyi baz alip merge yapmak istediyimize bagli\n",
    "\n",
    "# outer - olanda her iki dataframe in deyerlerini umumi baz alir, bir birine uygun gelmeyen row lara NaN qoyur\n",
    "# left - olanda left side daki dataframe i baz alir, onun butun deyerlerini alir, o birinin qarsiliqsiz deyerleri NaN olur\n",
    "# Right - da left in tersi\n",
    "# inner - de ise her ikisindeki ortaq rowlari goturur, bir birine uymayan rowlari almir\n",
    "pd.merge(df1_left,df2_right, how = 'inner', on = ['ortaq_col1'],['ortaq_col2']]) # df1_left ve df2_right datasetlerinin\n",
    "                                                                 # ortaq 2 ve cox column uzerinden merge edir \n",
    "\n",
    "pd.merge(df1_left,df2_right, how = 'outer', left_on= ['col1'], right_on = ['col2']) # ortaq olmayan col1 ve col2 uzerinden \n",
    "                                                                                        # de merge yapabiliriz\n",
    "    \n",
    "# Join --> DataFrame in methodudur , indexleri baz alaraq onlarin uzerinden islem yapar , bunun defaultu left dir\n",
    "\n",
    "df1.join(df2) # df1 i esas goturub onun indexlerini baz alib onlarin uzerinden join yapicak, df2 ye qarsiliq gelmeyen \n",
    "                # indexlere NaN gelecek\n",
    "\n",
    "df1.join(df2, how = 'outer') # buzaman butun indexler baz alinbacaq bir birine uygun gelmeyenler NaN alacaq\n",
    "\n",
    "# join de isler biraz qarisiqdir, eger iki DF imizde ortaq column varsa suffix yazmasaq error verir\n",
    "df1.join(df2, how = 'outer', lsuffix = '_left', rsuffix = \"_right\") # yeni bunlarin ortaq sutunlarini bir birinden ayird \n",
    "                # etmek ucun lsuffix (leftdek DF),rsuffux(right daki DF) in icine strig yazaraq columnlarin adini deyisirik\n",
    "\n",
    "# suffix lerle ugrasmadan da yapa biliriz set_index deyerek\n",
    "df1.set_index( 'ortaq_col' ).join( df2.set_index( 'ortaq_col' ))\n",
    "# bu da basqa yontem\n",
    "df1.join(df2.set_index('ortaq_col'), on =\"ortaq_col\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data da uygulana bilir\n",
    "df2[\"text\"] = df2[\"text\"].apply(lambda x: pd.Series(x).str.replace(\"(@+\\w+)\", \"\"))  # \\w typically matches [A-Za-z0-9_]\n",
    "df2[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e208c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't specified \"index_col=0\", you can not drop duplicates.\n",
    "df=pd.read_csv('Online Retail.csv',index_col=0)\n",
    "\n",
    "\n",
    "# Eshsiz (unique) urun sayisi kacdir\n",
    "df['Stockcode'].nunique()\n",
    "\n",
    "# hangi urunden kacar tane vardir\n",
    "df['Stockcode'].value_counts().head()\n",
    "\n",
    "# en cok sifaris edilen urunu coktan aza dogru siralayin\n",
    "df['Stockcode'].value_counts().sort_values(ascending = False).head()\n",
    "\n",
    "# faturalardfaki 'C' iptal edilen islemleri gostermektedir.Iptal edilen islemleri veri setinden cikartiniz (C536379 gibi)\n",
    "df = df[~df['Invoice'].str.contains('C', na =False)]\n",
    "\n",
    "# kac tanesi iptal olmus ve olmamis onun yuzdesini veririyor\n",
    "df[\"invoiceno\"].str.startswith('C').value_counts(normalize=True)*100\n",
    "\n",
    "# 'Invoiceno'lari 'C' ile baslayan ve onlara qarsiliq gelen 'quantity', 'unitprice' columnlarin deyerlerini getirir\n",
    "df[df[\"invoiceno\"].str.startswith('C')] [[\"invoiceno\",\"quantity\", \"unitprice\"]]\n",
    "\n",
    "# toplam kazanci ifade eden 'Total Price' columnu olustur\n",
    "df['Total_Price'] = df['Quantity'] * df['Price']\n",
    "\n",
    "# Country lari Total price coxdan aza olmaqla siralara\n",
    "df.groupby('country')[[\"total_price\"]].sum().sort_values(by=\"total_price\", ascending=False)\n",
    "\n",
    "# # Now we will drop NaN values of only \"customerid\" column. ('customerid' nin icindeki NaN lardan ibaret row lari\n",
    "                                                # drop etdik, o row larda diger columnlara aid olan deyerlerde silindi )\n",
    "df = df.dropna(subset=['customerid'], axis =1)\n",
    "\n",
    "\n",
    "# Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns\n",
    "\n",
    "df[df['quantity'] < 0].shape\n",
    "(8872, 8)\n",
    "\n",
    "df[\"quantity\"].min(), df[\"quantity\"].max()\n",
    "(-80995.0, 80995.0)\n",
    "\n",
    "df[df['unitprice'] < 0].shape\n",
    "(0, 8)\n",
    "\n",
    "df[\"unitprice\"].min(), df[\"unitprice\"].max()\n",
    "(0.0, 38970.0)\n",
    "\n",
    "# We did not see any negative value in \"unitprice\" but we have \"zero\" values. \n",
    "# These are not cancelled ones, because their ID's are not starting with \"C\" letter.\n",
    "# High probability these are gifts.\n",
    "\n",
    "df[(df.unitprice == 0)].shape\n",
    "(40, 8)\n",
    "\n",
    "\n",
    "## MATPLOTLIB\n",
    "plt.figure(figsize=(16,9))\n",
    "df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).plot.bar();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Useful Functions\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def missing_values(df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n",
    "    return missing_values[missing_values['Missing_Number']>0]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def first_looking(df):\n",
    "    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']),\n",
    "          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n",
    "    print(df.info(), '\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n",
    "\n",
    "    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    \n",
    "        \n",
    "def multicolinearity_control(df):\n",
    "    feature =[]\n",
    "    collinear=[]\n",
    "    for col in df.corr().columns:\n",
    "        for i in df.corr().index:\n",
    "            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n",
    "                    feature.append(col)\n",
    "                    collinear.append(i)\n",
    "                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n",
    "                                  \"red\", attrs=['bold']), df.shape,'\\n',\n",
    "                                  colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n",
    "    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n",
    "    if duplicate_values > 0:\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(duplicate_values, colored(\"Duplicates were dropped!\"),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    else:\n",
    "        print(colored(\"There are no duplicates\"),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')     \n",
    "        \n",
    "def drop_columns(df, drop_columns):\n",
    "    if drop_columns !=[]:\n",
    "        df.drop(drop_columns, axis=1, inplace=True)\n",
    "        print(drop_columns, 'were dropped')\n",
    "    else:\n",
    "        print(colored('We will now check the missing values and if necessary will drop realted columns!', attrs=['bold']),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "        \n",
    "def drop_null(df, limit):\n",
    "    print('Shape:', df.shape)\n",
    "    for i in df.isnull().sum().index:\n",
    "        if (df.isnull().sum()[i]/df.shape[0]*100)>limit:\n",
    "            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "            print('new shape:', df.shape)       \n",
    "    print('New shape after missing value control:', df.shape)\n",
    "    \n",
    "###############################################################################\n",
    "first_looking(df)\n",
    "duplicate_values(df)\n",
    "drop_columns(df,[])\n",
    "drop_null(df, 90)\n",
    "# df.head()\n",
    "# df.tail()\n",
    "# df.sample(5)\n",
    "# df.describe().T\n",
    "# df.describe(include=object).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8375fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv dosyasini kayd etmek...uzerinde islem yapdigimiz her hangi bir datasetini csv kimi save etmek istesek...\n",
    "\n",
    "df.to_csv('new_csv_name') # sonra bunu oxuduruq(elave index gelmemesi ucun burada da index = False yaza bilrik, ya bunu edirik yada asagidakini)\n",
    "\n",
    "df = pd.read_csv('new_csv_name' , index_col = 0) # index_col = 0 qoymasaq dataframemimizde elave indexler yazilacaq(ikiqat\n",
    "                                                # yan-yana,onu onlemek icin)\n",
    "\n",
    "# csv dosyasinda bezen okutdugumuzda verilerin arasinda objectler ola biler(mis ucun \\t kimi) onlari aradan goturmek ucun\n",
    "df = pd.read_csv('titanik.csv', sep = '\\t') # shift tab yapdigimizda gore bilerik\n",
    "\n",
    "# bezen de csv ini oktdugumuzda icinde columlar onlara aid olan value lar qarisiq (aralarinda isareler falan)gorune biler\n",
    "# onu anlamaq ucun\n",
    "with open('ornekcsv.csv','r', encoding = 'uft-8') as f:\n",
    "    print(f.read())\n",
    "# isareleri aradan kaldirmak ucun yene(mis: bu sefer aralarinda ; kimi)\n",
    "df = pd.read_csv('ornekcsv.csv', sep = ';') \n",
    "\n",
    "\n",
    "# bazen calisacagimiz dosyada bos satirlar olucak,onlarin DataFrame de gorunur hale gelmesini istersek...\n",
    "pd.read_csv('file_name.csv', skip_blank_lines = False) # bunun defaultu True dur, shift tab yapib bakicaz unutursak\n",
    "\n",
    "# bezen ola biler ki DataFrame i acanda cedveli list icinde vere biler...\n",
    "df[0] # yazsaq normal DataFrame kimi olacaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1f337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf98de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb99aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f638a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e979a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b404f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb765b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
